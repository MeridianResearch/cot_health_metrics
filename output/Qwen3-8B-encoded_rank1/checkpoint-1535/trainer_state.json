{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 1535,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03257328990228013,
      "grad_norm": 252.16375732421875,
      "learning_rate": 1.168831168831169e-06,
      "loss": 4.9374,
      "step": 10
    },
    {
      "epoch": 0.06514657980456026,
      "grad_norm": 97.28551483154297,
      "learning_rate": 2.4675324675324676e-06,
      "loss": 3.9326,
      "step": 20
    },
    {
      "epoch": 0.09771986970684039,
      "grad_norm": 22.48785400390625,
      "learning_rate": 3.7662337662337666e-06,
      "loss": 3.0405,
      "step": 30
    },
    {
      "epoch": 0.13029315960912052,
      "grad_norm": 22.077709197998047,
      "learning_rate": 5.064935064935065e-06,
      "loss": 2.5386,
      "step": 40
    },
    {
      "epoch": 0.16286644951140064,
      "grad_norm": 28.06684684753418,
      "learning_rate": 6.363636363636364e-06,
      "loss": 2.2838,
      "step": 50
    },
    {
      "epoch": 0.19543973941368079,
      "grad_norm": 35.633609771728516,
      "learning_rate": 7.662337662337663e-06,
      "loss": 1.8988,
      "step": 60
    },
    {
      "epoch": 0.2280130293159609,
      "grad_norm": 29.50445556640625,
      "learning_rate": 8.96103896103896e-06,
      "loss": 1.5977,
      "step": 70
    },
    {
      "epoch": 0.26058631921824105,
      "grad_norm": 28.836402893066406,
      "learning_rate": 1.025974025974026e-05,
      "loss": 1.3836,
      "step": 80
    },
    {
      "epoch": 0.2931596091205212,
      "grad_norm": 22.961820602416992,
      "learning_rate": 1.155844155844156e-05,
      "loss": 1.2979,
      "step": 90
    },
    {
      "epoch": 0.3257328990228013,
      "grad_norm": 23.274866104125977,
      "learning_rate": 1.2857142857142859e-05,
      "loss": 1.1023,
      "step": 100
    },
    {
      "epoch": 0.3583061889250814,
      "grad_norm": 23.001588821411133,
      "learning_rate": 1.4155844155844157e-05,
      "loss": 1.0718,
      "step": 110
    },
    {
      "epoch": 0.39087947882736157,
      "grad_norm": 22.276561737060547,
      "learning_rate": 1.5454545454545454e-05,
      "loss": 0.966,
      "step": 120
    },
    {
      "epoch": 0.4234527687296417,
      "grad_norm": 18.724475860595703,
      "learning_rate": 1.6753246753246756e-05,
      "loss": 0.9104,
      "step": 130
    },
    {
      "epoch": 0.4560260586319218,
      "grad_norm": 27.511959075927734,
      "learning_rate": 1.8051948051948053e-05,
      "loss": 0.8374,
      "step": 140
    },
    {
      "epoch": 0.48859934853420195,
      "grad_norm": 24.720609664916992,
      "learning_rate": 1.9350649350649354e-05,
      "loss": 0.7575,
      "step": 150
    },
    {
      "epoch": 0.5211726384364821,
      "grad_norm": 23.429750442504883,
      "learning_rate": 1.9999353128944386e-05,
      "loss": 0.6756,
      "step": 160
    },
    {
      "epoch": 0.5537459283387622,
      "grad_norm": 20.776611328125,
      "learning_rate": 1.9994178662619237e-05,
      "loss": 0.6701,
      "step": 170
    },
    {
      "epoch": 0.5863192182410424,
      "grad_norm": 17.9052791595459,
      "learning_rate": 1.9983832407652323e-05,
      "loss": 0.6295,
      "step": 180
    },
    {
      "epoch": 0.6188925081433225,
      "grad_norm": 18.296236038208008,
      "learning_rate": 1.9968319718024776e-05,
      "loss": 0.6208,
      "step": 190
    },
    {
      "epoch": 0.6514657980456026,
      "grad_norm": 17.020713806152344,
      "learning_rate": 1.994764862124487e-05,
      "loss": 0.607,
      "step": 200
    },
    {
      "epoch": 0.6840390879478827,
      "grad_norm": 20.235633850097656,
      "learning_rate": 1.9921829814193987e-05,
      "loss": 0.6055,
      "step": 210
    },
    {
      "epoch": 0.7166123778501629,
      "grad_norm": 20.4251766204834,
      "learning_rate": 1.9890876657591146e-05,
      "loss": 0.59,
      "step": 220
    },
    {
      "epoch": 0.749185667752443,
      "grad_norm": 17.84681510925293,
      "learning_rate": 1.9854805169079142e-05,
      "loss": 0.5492,
      "step": 230
    },
    {
      "epoch": 0.7817589576547231,
      "grad_norm": 20.34191131591797,
      "learning_rate": 1.9813634014935697e-05,
      "loss": 0.6009,
      "step": 240
    },
    {
      "epoch": 0.8143322475570033,
      "grad_norm": 21.223358154296875,
      "learning_rate": 1.9767384500414054e-05,
      "loss": 0.5443,
      "step": 250
    },
    {
      "epoch": 0.8469055374592834,
      "grad_norm": 16.601150512695312,
      "learning_rate": 1.971608055871793e-05,
      "loss": 0.613,
      "step": 260
    },
    {
      "epoch": 0.8794788273615635,
      "grad_norm": 19.705947875976562,
      "learning_rate": 1.965974873861655e-05,
      "loss": 0.572,
      "step": 270
    },
    {
      "epoch": 0.9120521172638436,
      "grad_norm": 20.08797264099121,
      "learning_rate": 1.95984181907062e-05,
      "loss": 0.5476,
      "step": 280
    },
    {
      "epoch": 0.9446254071661238,
      "grad_norm": 18.425168991088867,
      "learning_rate": 1.9532120652325362e-05,
      "loss": 0.5688,
      "step": 290
    },
    {
      "epoch": 0.9771986970684039,
      "grad_norm": 19.6461238861084,
      "learning_rate": 1.946089043113128e-05,
      "loss": 0.5349,
      "step": 300
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.507222056388855,
      "eval_runtime": 12.2297,
      "eval_samples_per_second": 7.277,
      "eval_steps_per_second": 7.277,
      "step": 307
    },
    {
      "epoch": 1.009771986970684,
      "grad_norm": 15.749368667602539,
      "learning_rate": 1.938476438734642e-05,
      "loss": 0.4902,
      "step": 310
    },
    {
      "epoch": 1.0423452768729642,
      "grad_norm": 16.850902557373047,
      "learning_rate": 1.9303781914684057e-05,
      "loss": 0.5169,
      "step": 320
    },
    {
      "epoch": 1.0749185667752443,
      "grad_norm": 18.807693481445312,
      "learning_rate": 1.921798491996279e-05,
      "loss": 0.5069,
      "step": 330
    },
    {
      "epoch": 1.1074918566775245,
      "grad_norm": 14.197159767150879,
      "learning_rate": 1.912741780142061e-05,
      "loss": 0.4598,
      "step": 340
    },
    {
      "epoch": 1.1400651465798046,
      "grad_norm": 15.896628379821777,
      "learning_rate": 1.9032127425739695e-05,
      "loss": 0.512,
      "step": 350
    },
    {
      "epoch": 1.1726384364820848,
      "grad_norm": 14.960968017578125,
      "learning_rate": 1.8932163103793832e-05,
      "loss": 0.4986,
      "step": 360
    },
    {
      "epoch": 1.205211726384365,
      "grad_norm": 14.70835018157959,
      "learning_rate": 1.882757656513101e-05,
      "loss": 0.4588,
      "step": 370
    },
    {
      "epoch": 1.237785016286645,
      "grad_norm": 20.98525047302246,
      "learning_rate": 1.871842193120444e-05,
      "loss": 0.4814,
      "step": 380
    },
    {
      "epoch": 1.2703583061889252,
      "grad_norm": 21.195186614990234,
      "learning_rate": 1.8604755687365728e-05,
      "loss": 0.4821,
      "step": 390
    },
    {
      "epoch": 1.3029315960912053,
      "grad_norm": 19.49863624572754,
      "learning_rate": 1.848663665363489e-05,
      "loss": 0.4711,
      "step": 400
    },
    {
      "epoch": 1.3355048859934853,
      "grad_norm": 18.59563636779785,
      "learning_rate": 1.8364125954262112e-05,
      "loss": 0.4632,
      "step": 410
    },
    {
      "epoch": 1.3680781758957654,
      "grad_norm": 16.981237411499023,
      "learning_rate": 1.823728698609719e-05,
      "loss": 0.4732,
      "step": 420
    },
    {
      "epoch": 1.4006514657980456,
      "grad_norm": 16.530620574951172,
      "learning_rate": 1.8106185385782916e-05,
      "loss": 0.4857,
      "step": 430
    },
    {
      "epoch": 1.4332247557003257,
      "grad_norm": 15.553412437438965,
      "learning_rate": 1.7970888995789427e-05,
      "loss": 0.5008,
      "step": 440
    },
    {
      "epoch": 1.4657980456026058,
      "grad_norm": 17.767431259155273,
      "learning_rate": 1.783146782930706e-05,
      "loss": 0.4826,
      "step": 450
    },
    {
      "epoch": 1.498371335504886,
      "grad_norm": 15.902400016784668,
      "learning_rate": 1.7687994034015943e-05,
      "loss": 0.492,
      "step": 460
    },
    {
      "epoch": 1.5309446254071661,
      "grad_norm": 15.694785118103027,
      "learning_rate": 1.7540541854750984e-05,
      "loss": 0.469,
      "step": 470
    },
    {
      "epoch": 1.5635179153094463,
      "grad_norm": 15.2349853515625,
      "learning_rate": 1.738918759508166e-05,
      "loss": 0.4801,
      "step": 480
    },
    {
      "epoch": 1.5960912052117264,
      "grad_norm": 16.28590202331543,
      "learning_rate": 1.7234009577826428e-05,
      "loss": 0.5097,
      "step": 490
    },
    {
      "epoch": 1.6286644951140063,
      "grad_norm": 15.411308288574219,
      "learning_rate": 1.7075088104522235e-05,
      "loss": 0.4452,
      "step": 500
    },
    {
      "epoch": 1.6612377850162865,
      "grad_norm": 13.670369148254395,
      "learning_rate": 1.6912505413870054e-05,
      "loss": 0.5032,
      "step": 510
    },
    {
      "epoch": 1.6938110749185666,
      "grad_norm": 15.885871887207031,
      "learning_rate": 1.6746345639178013e-05,
      "loss": 0.4526,
      "step": 520
    },
    {
      "epoch": 1.7263843648208468,
      "grad_norm": 14.363598823547363,
      "learning_rate": 1.657669476482407e-05,
      "loss": 0.4744,
      "step": 530
    },
    {
      "epoch": 1.758957654723127,
      "grad_norm": 14.62663459777832,
      "learning_rate": 1.640364058176079e-05,
      "loss": 0.4905,
      "step": 540
    },
    {
      "epoch": 1.791530944625407,
      "grad_norm": 16.907697677612305,
      "learning_rate": 1.6227272642085326e-05,
      "loss": 0.4659,
      "step": 550
    },
    {
      "epoch": 1.8241042345276872,
      "grad_norm": 13.50130558013916,
      "learning_rate": 1.6047682212697925e-05,
      "loss": 0.4531,
      "step": 560
    },
    {
      "epoch": 1.8566775244299674,
      "grad_norm": 14.894065856933594,
      "learning_rate": 1.586496222807318e-05,
      "loss": 0.4786,
      "step": 570
    },
    {
      "epoch": 1.8892508143322475,
      "grad_norm": 15.265202522277832,
      "learning_rate": 1.5679207242168234e-05,
      "loss": 0.4644,
      "step": 580
    },
    {
      "epoch": 1.9218241042345277,
      "grad_norm": 17.756942749023438,
      "learning_rate": 1.549051337949305e-05,
      "loss": 0.4655,
      "step": 590
    },
    {
      "epoch": 1.9543973941368078,
      "grad_norm": 15.96093463897705,
      "learning_rate": 1.5298978285367826e-05,
      "loss": 0.4766,
      "step": 600
    },
    {
      "epoch": 1.986970684039088,
      "grad_norm": 13.340731620788574,
      "learning_rate": 1.510470107539353e-05,
      "loss": 0.4502,
      "step": 610
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.47416096925735474,
      "eval_runtime": 12.2207,
      "eval_samples_per_second": 7.283,
      "eval_steps_per_second": 7.283,
      "step": 614
    },
    {
      "epoch": 2.019543973941368,
      "grad_norm": 21.967151641845703,
      "learning_rate": 1.4907782284161531e-05,
      "loss": 0.3819,
      "step": 620
    },
    {
      "epoch": 2.0521172638436482,
      "grad_norm": 19.451026916503906,
      "learning_rate": 1.4708323813228949e-05,
      "loss": 0.3511,
      "step": 630
    },
    {
      "epoch": 2.0846905537459284,
      "grad_norm": 16.318830490112305,
      "learning_rate": 1.4506428878386623e-05,
      "loss": 0.3735,
      "step": 640
    },
    {
      "epoch": 2.1172638436482085,
      "grad_norm": 19.016883850097656,
      "learning_rate": 1.4302201956247e-05,
      "loss": 0.38,
      "step": 650
    },
    {
      "epoch": 2.1498371335504887,
      "grad_norm": 23.61890411376953,
      "learning_rate": 1.4095748730179559e-05,
      "loss": 0.3588,
      "step": 660
    },
    {
      "epoch": 2.182410423452769,
      "grad_norm": 20.315141677856445,
      "learning_rate": 1.3887176035621777e-05,
      "loss": 0.3669,
      "step": 670
    },
    {
      "epoch": 2.214983713355049,
      "grad_norm": 17.763628005981445,
      "learning_rate": 1.367659180479391e-05,
      "loss": 0.3538,
      "step": 680
    },
    {
      "epoch": 2.247557003257329,
      "grad_norm": 19.86139488220215,
      "learning_rate": 1.3464105010846222e-05,
      "loss": 0.3399,
      "step": 690
    },
    {
      "epoch": 2.2801302931596092,
      "grad_norm": 17.4810848236084,
      "learning_rate": 1.3249825611467535e-05,
      "loss": 0.3458,
      "step": 700
    },
    {
      "epoch": 2.3127035830618894,
      "grad_norm": 18.634437561035156,
      "learning_rate": 1.3033864491984324e-05,
      "loss": 0.3771,
      "step": 710
    },
    {
      "epoch": 2.3452768729641695,
      "grad_norm": 18.266103744506836,
      "learning_rate": 1.2816333407979775e-05,
      "loss": 0.3708,
      "step": 720
    },
    {
      "epoch": 2.3778501628664497,
      "grad_norm": 18.855257034301758,
      "learning_rate": 1.259734492746246e-05,
      "loss": 0.3731,
      "step": 730
    },
    {
      "epoch": 2.41042345276873,
      "grad_norm": 20.528955459594727,
      "learning_rate": 1.2377012372614676e-05,
      "loss": 0.3674,
      "step": 740
    },
    {
      "epoch": 2.44299674267101,
      "grad_norm": 19.469402313232422,
      "learning_rate": 1.2155449761150431e-05,
      "loss": 0.3823,
      "step": 750
    },
    {
      "epoch": 2.47557003257329,
      "grad_norm": 21.082387924194336,
      "learning_rate": 1.1932771747313574e-05,
      "loss": 0.3552,
      "step": 760
    },
    {
      "epoch": 2.5081433224755703,
      "grad_norm": 18.55885124206543,
      "learning_rate": 1.1709093562546483e-05,
      "loss": 0.3642,
      "step": 770
    },
    {
      "epoch": 2.5407166123778504,
      "grad_norm": 18.56386947631836,
      "learning_rate": 1.148453095586011e-05,
      "loss": 0.3927,
      "step": 780
    },
    {
      "epoch": 2.5732899022801305,
      "grad_norm": 17.3785457611084,
      "learning_rate": 1.1259200133936142e-05,
      "loss": 0.3585,
      "step": 790
    },
    {
      "epoch": 2.6058631921824107,
      "grad_norm": 19.13262176513672,
      "learning_rate": 1.1033217700992373e-05,
      "loss": 0.3448,
      "step": 800
    },
    {
      "epoch": 2.6384364820846904,
      "grad_norm": 22.656635284423828,
      "learning_rate": 1.0806700598442343e-05,
      "loss": 0.3622,
      "step": 810
    },
    {
      "epoch": 2.6710097719869705,
      "grad_norm": 20.519161224365234,
      "learning_rate": 1.0579766044380473e-05,
      "loss": 0.3705,
      "step": 820
    },
    {
      "epoch": 2.7035830618892507,
      "grad_norm": 18.810272216796875,
      "learning_rate": 1.0352531472924035e-05,
      "loss": 0.3271,
      "step": 830
    },
    {
      "epoch": 2.736156351791531,
      "grad_norm": 24.193113327026367,
      "learning_rate": 1.0125114473443332e-05,
      "loss": 0.3623,
      "step": 840
    },
    {
      "epoch": 2.768729641693811,
      "grad_norm": 17.487024307250977,
      "learning_rate": 9.897632729711553e-06,
      "loss": 0.3753,
      "step": 850
    },
    {
      "epoch": 2.801302931596091,
      "grad_norm": 21.188617706298828,
      "learning_rate": 9.670203959005733e-06,
      "loss": 0.3611,
      "step": 860
    },
    {
      "epoch": 2.8338762214983713,
      "grad_norm": 21.402679443359375,
      "learning_rate": 9.442945851190433e-06,
      "loss": 0.3452,
      "step": 870
    },
    {
      "epoch": 2.8664495114006514,
      "grad_norm": 15.506099700927734,
      "learning_rate": 9.215976007815553e-06,
      "loss": 0.355,
      "step": 880
    },
    {
      "epoch": 2.8990228013029316,
      "grad_norm": 20.053592681884766,
      "learning_rate": 8.989411881259888e-06,
      "loss": 0.3552,
      "step": 890
    },
    {
      "epoch": 2.9315960912052117,
      "grad_norm": 21.539634704589844,
      "learning_rate": 8.763370713951841e-06,
      "loss": 0.3416,
      "step": 900
    },
    {
      "epoch": 2.964169381107492,
      "grad_norm": 17.506500244140625,
      "learning_rate": 8.537969477698822e-06,
      "loss": 0.3416,
      "step": 910
    },
    {
      "epoch": 2.996742671009772,
      "grad_norm": 20.138526916503906,
      "learning_rate": 8.313324813156633e-06,
      "loss": 0.3911,
      "step": 920
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.4933302402496338,
      "eval_runtime": 12.1828,
      "eval_samples_per_second": 7.305,
      "eval_steps_per_second": 7.305,
      "step": 921
    },
    {
      "epoch": 3.029315960912052,
      "grad_norm": 21.49272346496582,
      "learning_rate": 8.08955296947028e-06,
      "loss": 0.2751,
      "step": 930
    },
    {
      "epoch": 3.0618892508143323,
      "grad_norm": 27.74132537841797,
      "learning_rate": 7.866769744117332e-06,
      "loss": 0.2439,
      "step": 940
    },
    {
      "epoch": 3.0944625407166124,
      "grad_norm": 20.827167510986328,
      "learning_rate": 7.645090422985035e-06,
      "loss": 0.2345,
      "step": 950
    },
    {
      "epoch": 3.1270358306188926,
      "grad_norm": 20.678539276123047,
      "learning_rate": 7.4246297207121695e-06,
      "loss": 0.2382,
      "step": 960
    },
    {
      "epoch": 3.1596091205211727,
      "grad_norm": 15.684924125671387,
      "learning_rate": 7.205501721326491e-06,
      "loss": 0.2534,
      "step": 970
    },
    {
      "epoch": 3.192182410423453,
      "grad_norm": 24.287395477294922,
      "learning_rate": 6.987819819208531e-06,
      "loss": 0.2455,
      "step": 980
    },
    {
      "epoch": 3.224755700325733,
      "grad_norm": 25.154361724853516,
      "learning_rate": 6.771696660412242e-06,
      "loss": 0.2443,
      "step": 990
    },
    {
      "epoch": 3.257328990228013,
      "grad_norm": 22.41973304748535,
      "learning_rate": 6.557244084372922e-06,
      "loss": 0.2512,
      "step": 1000
    },
    {
      "epoch": 3.2899022801302933,
      "grad_norm": 21.956289291381836,
      "learning_rate": 6.344573066032524e-06,
      "loss": 0.2478,
      "step": 1010
    },
    {
      "epoch": 3.3224755700325734,
      "grad_norm": 21.168621063232422,
      "learning_rate": 6.133793658412339e-06,
      "loss": 0.2323,
      "step": 1020
    },
    {
      "epoch": 3.3550488599348536,
      "grad_norm": 23.777374267578125,
      "learning_rate": 5.925014935662728e-06,
      "loss": 0.2673,
      "step": 1030
    },
    {
      "epoch": 3.3876221498371337,
      "grad_norm": 22.215843200683594,
      "learning_rate": 5.71834493661944e-06,
      "loss": 0.2474,
      "step": 1040
    },
    {
      "epoch": 3.420195439739414,
      "grad_norm": 28.848892211914062,
      "learning_rate": 5.5138906088956514e-06,
      "loss": 0.2473,
      "step": 1050
    },
    {
      "epoch": 3.4527687296416936,
      "grad_norm": 26.51279640197754,
      "learning_rate": 5.311757753538711e-06,
      "loss": 0.2565,
      "step": 1060
    },
    {
      "epoch": 3.4853420195439737,
      "grad_norm": 24.217735290527344,
      "learning_rate": 5.1120509702802e-06,
      "loss": 0.2347,
      "step": 1070
    },
    {
      "epoch": 3.517915309446254,
      "grad_norm": 26.431486129760742,
      "learning_rate": 4.914873603407649e-06,
      "loss": 0.252,
      "step": 1080
    },
    {
      "epoch": 3.550488599348534,
      "grad_norm": 24.104650497436523,
      "learning_rate": 4.720327688285922e-06,
      "loss": 0.2602,
      "step": 1090
    },
    {
      "epoch": 3.583061889250814,
      "grad_norm": 24.55135154724121,
      "learning_rate": 4.52851389855596e-06,
      "loss": 0.2433,
      "step": 1100
    },
    {
      "epoch": 3.6156351791530943,
      "grad_norm": 24.13320541381836,
      "learning_rate": 4.3395314940381536e-06,
      "loss": 0.256,
      "step": 1110
    },
    {
      "epoch": 3.6482084690553744,
      "grad_norm": 23.71819496154785,
      "learning_rate": 4.153478269367381e-06,
      "loss": 0.2257,
      "step": 1120
    },
    {
      "epoch": 3.6807817589576546,
      "grad_norm": 21.3315372467041,
      "learning_rate": 3.970450503386218e-06,
      "loss": 0.2466,
      "step": 1130
    },
    {
      "epoch": 3.7133550488599347,
      "grad_norm": 22.604848861694336,
      "learning_rate": 3.7905429093225477e-06,
      "loss": 0.249,
      "step": 1140
    },
    {
      "epoch": 3.745928338762215,
      "grad_norm": 27.328702926635742,
      "learning_rate": 3.613848585777363e-06,
      "loss": 0.2548,
      "step": 1150
    },
    {
      "epoch": 3.778501628664495,
      "grad_norm": 29.051040649414062,
      "learning_rate": 3.440458968548085e-06,
      "loss": 0.2741,
      "step": 1160
    },
    {
      "epoch": 3.811074918566775,
      "grad_norm": 25.59578514099121,
      "learning_rate": 3.27046378331236e-06,
      "loss": 0.2381,
      "step": 1170
    },
    {
      "epoch": 3.8436482084690553,
      "grad_norm": 24.00312614440918,
      "learning_rate": 3.103950999196792e-06,
      "loss": 0.2241,
      "step": 1180
    },
    {
      "epoch": 3.8762214983713354,
      "grad_norm": 30.059131622314453,
      "learning_rate": 2.9410067832546973e-06,
      "loss": 0.2568,
      "step": 1190
    },
    {
      "epoch": 3.9087947882736156,
      "grad_norm": 22.282562255859375,
      "learning_rate": 2.7817154558763393e-06,
      "loss": 0.2547,
      "step": 1200
    },
    {
      "epoch": 3.9413680781758957,
      "grad_norm": 25.889070510864258,
      "learning_rate": 2.6261594471548278e-06,
      "loss": 0.2593,
      "step": 1210
    },
    {
      "epoch": 3.973941368078176,
      "grad_norm": 23.42925453186035,
      "learning_rate": 2.4744192542301636e-06,
      "loss": 0.2291,
      "step": 1220
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.5837703943252563,
      "eval_runtime": 11.9976,
      "eval_samples_per_second": 7.418,
      "eval_steps_per_second": 7.418,
      "step": 1228
    },
    {
      "epoch": 4.006514657980456,
      "grad_norm": 20.823089599609375,
      "learning_rate": 2.3265733996335926e-06,
      "loss": 0.2489,
      "step": 1230
    },
    {
      "epoch": 4.039087947882736,
      "grad_norm": 22.631567001342773,
      "learning_rate": 2.1826983906537415e-06,
      "loss": 0.1734,
      "step": 1240
    },
    {
      "epoch": 4.071661237785016,
      "grad_norm": 28.461698532104492,
      "learning_rate": 2.0428686797456265e-06,
      "loss": 0.1815,
      "step": 1250
    },
    {
      "epoch": 4.1042345276872965,
      "grad_norm": 53.95158767700195,
      "learning_rate": 1.9071566260029772e-06,
      "loss": 0.1681,
      "step": 1260
    },
    {
      "epoch": 4.136807817589577,
      "grad_norm": 27.279399871826172,
      "learning_rate": 1.7756324577138627e-06,
      "loss": 0.1719,
      "step": 1270
    },
    {
      "epoch": 4.169381107491857,
      "grad_norm": 28.60346794128418,
      "learning_rate": 1.6483642360189388e-06,
      "loss": 0.1646,
      "step": 1280
    },
    {
      "epoch": 4.201954397394137,
      "grad_norm": 25.59531021118164,
      "learning_rate": 1.5254178196911552e-06,
      "loss": 0.1672,
      "step": 1290
    },
    {
      "epoch": 4.234527687296417,
      "grad_norm": 26.36968421936035,
      "learning_rate": 1.40685683105517e-06,
      "loss": 0.1727,
      "step": 1300
    },
    {
      "epoch": 4.267100977198697,
      "grad_norm": 28.609657287597656,
      "learning_rate": 1.2927426230640417e-06,
      "loss": 0.1766,
      "step": 1310
    },
    {
      "epoch": 4.299674267100977,
      "grad_norm": 23.79869842529297,
      "learning_rate": 1.1831342475503105e-06,
      "loss": 0.1696,
      "step": 1320
    },
    {
      "epoch": 4.3322475570032575,
      "grad_norm": 24.114171981811523,
      "learning_rate": 1.0780884246678447e-06,
      "loss": 0.1756,
      "step": 1330
    },
    {
      "epoch": 4.364820846905538,
      "grad_norm": 24.818269729614258,
      "learning_rate": 9.776595135402899e-07,
      "loss": 0.1695,
      "step": 1340
    },
    {
      "epoch": 4.397394136807818,
      "grad_norm": 28.07831382751465,
      "learning_rate": 8.818994841313011e-07,
      "loss": 0.1612,
      "step": 1350
    },
    {
      "epoch": 4.429967426710098,
      "grad_norm": 25.779888153076172,
      "learning_rate": 7.908578903511255e-07,
      "loss": 0.1702,
      "step": 1360
    },
    {
      "epoch": 4.462540716612378,
      "grad_norm": 31.354713439941406,
      "learning_rate": 7.045818444134356e-07,
      "loss": 0.171,
      "step": 1370
    },
    {
      "epoch": 4.495114006514658,
      "grad_norm": 20.898719787597656,
      "learning_rate": 6.231159924556973e-07,
      "loss": 0.173,
      "step": 1380
    },
    {
      "epoch": 4.527687296416938,
      "grad_norm": 23.91171646118164,
      "learning_rate": 5.465024914356842e-07,
      "loss": 0.164,
      "step": 1390
    },
    {
      "epoch": 4.5602605863192185,
      "grad_norm": 23.691204071044922,
      "learning_rate": 4.747809873160858e-07,
      "loss": 0.1574,
      "step": 1400
    },
    {
      "epoch": 4.592833876221499,
      "grad_norm": 29.764854431152344,
      "learning_rate": 4.0798859454851405e-07,
      "loss": 0.1731,
      "step": 1410
    },
    {
      "epoch": 4.625407166123779,
      "grad_norm": 25.216535568237305,
      "learning_rate": 3.461598768675123e-07,
      "loss": 0.1514,
      "step": 1420
    },
    {
      "epoch": 4.657980456026059,
      "grad_norm": 26.015369415283203,
      "learning_rate": 2.893268294045126e-07,
      "loss": 0.182,
      "step": 1430
    },
    {
      "epoch": 4.690553745928339,
      "grad_norm": 25.1239070892334,
      "learning_rate": 2.3751886213098097e-07,
      "loss": 0.1783,
      "step": 1440
    },
    {
      "epoch": 4.723127035830619,
      "grad_norm": 25.793819427490234,
      "learning_rate": 1.9076278463935893e-07,
      "loss": 0.1883,
      "step": 1450
    },
    {
      "epoch": 4.755700325732899,
      "grad_norm": 25.651748657226562,
      "learning_rate": 1.490827922696203e-07,
      "loss": 0.1673,
      "step": 1460
    },
    {
      "epoch": 4.7882736156351795,
      "grad_norm": 23.91143035888672,
      "learning_rate": 1.1250045358866645e-07,
      "loss": 0.195,
      "step": 1470
    },
    {
      "epoch": 4.82084690553746,
      "grad_norm": 24.95374870300293,
      "learning_rate": 8.103469922902785e-08,
      "loss": 0.1839,
      "step": 1480
    },
    {
      "epoch": 4.85342019543974,
      "grad_norm": 27.547739028930664,
      "learning_rate": 5.4701812092627925e-08,
      "loss": 0.175,
      "step": 1490
    },
    {
      "epoch": 4.88599348534202,
      "grad_norm": 23.730051040649414,
      "learning_rate": 3.351541892471777e-08,
      "loss": 0.1761,
      "step": 1500
    },
    {
      "epoch": 4.918566775244299,
      "grad_norm": 18.78740119934082,
      "learning_rate": 1.748648326229452e-08,
      "loss": 0.1723,
      "step": 1510
    },
    {
      "epoch": 4.95114006514658,
      "grad_norm": 23.615976333618164,
      "learning_rate": 6.623299760698532e-09,
      "loss": 0.1767,
      "step": 1520
    },
    {
      "epoch": 4.9837133550488595,
      "grad_norm": 27.70183563232422,
      "learning_rate": 9.31489901286886e-10,
      "loss": 0.1748,
      "step": 1530
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.6662209630012512,
      "eval_runtime": 11.9997,
      "eval_samples_per_second": 7.417,
      "eval_steps_per_second": 7.417,
      "step": 1535
    }
  ],
  "logging_steps": 10,
  "max_steps": 1535,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.8428518852438016e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
