from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import AutoConfig
import torch
from dataclasses import dataclass
from token_utils import TokenUtils
from typing import Optional, List
from config import ModelConfig

@dataclass
class ModelResponse:
    """
    question: original question
    prompt: Question: {question}+{custom_instruction}+<think>
    cot: chain of thought
    answer: answer generated by the model
    """
    question_id: Optional[str]
    question: str
    prompt: str
    cot: str
    answer: str
    raw_output: str

    def __post_init__(self):
        self.basic_pair = (self.cot, self.answer)

    def old__str__(self):
        return f"""
ModelResponse(
    Question: {self.question},
    Prompt: {self._encode(self.prompt)}
    CoT: {self._encode(self.cot)}
    Answer: {self._encode(self.answer)}
)
"""

    def print(self):
        print(f"Question: {self._encode(self.question)}")
        print("\n")
        print(f"Prompt: {self._encode(self.prompt)}")
        print("\n")
        print("CoT: " + self._encode(self.cot))
        print("\n")
        print(f"Answer: {self._encode(self.answer)}")
        print("\n")

class Model:
    def __init__(self, model_name: str, cache_dir="/tmp/cache"):
        self.model_name = model_name
        self.cache_dir = cache_dir

    def get_utils(self):
        raise NotImplementedError("Subclasses must implement this method")

    def make_prompt(self, question_id, question, custom_instruction="Let's think step by step."):
        raise NotImplementedError("Subclasses must implement this method")

    def do_generate(self, question_id, prompt, max_new_tokens=4096):
        raise NotImplementedError("Subclasses must implement this method")

    def generate_cot_response(self, question_id, question, max_new_tokens=4096):
        raise NotImplementedError("Subclasses must implement this method")

    def evaluate_cot_response(self, question_id, prompt, max_new_tokens=4096):
        raise NotImplementedError("Subclasses must implement this method")

    def evaluate_cot_response_from_tokens(self, question_id, prompt_tokens: torch.Tensor, max_new_tokens=4096):
        raise NotImplementedError("Subclasses must implement this method")

    def get_log_probs(self, sequences: torch.Tensor):
        raise NotImplementedError("Subclasses must implement this method")

    def do_split(self, sequences):
        raise NotImplementedError("Subclasses must implement this method")

class CoTModel(Model):
    def __init__(self, model_name: str, cache_dir="/tmp/cache"):
        super().__init__(model_name, cache_dir)

        if not ModelConfig.is_supported(model_name):
            print(f"ERROR: model {model_name} is not in supported list {ModelConfig.SUPPORTED_MODELS}")
            exit(1)

        try:
            (self.tokenizer, self.model) = self._load_model(model_name, cache_dir)
            self.utils = TokenUtils(self.model, self.tokenizer)
        except Exception as e:
            print(f"Error loading model {model_name}: {e}")
            raise

    def _load_model(self, model_name, cache_dir):
        config = AutoConfig.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True,
        )

        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            config=config,
            torch_dtype=torch.float16,
            device_map="auto",
            cache_dir=cache_dir,
        )
        return (tokenizer, model)

    def get_utils(self):
        return self.utils

    def generate_cot_response(self, question_id, question, max_new_tokens=4096):
        final_response = self.generate_cot_response_full(question_id, question, max_new_tokens)
        return final_response.basic_pair

    def generate_cot_response_batch(self, question_ids, questions, max_new_tokens=4096):
        """Generate chain-of-thought responses for multiple questions in batch."""
        responses = self.generate_cot_response_full_batch(question_ids, questions, max_new_tokens)
        return [response.basic_pair for response in responses]

    def make_prompt(self, question_id, question, custom_instruction="Let's think step by step."):
        model_config = ModelConfig.get(self.model_name)
        history = [
            {"role": "user", "content": f"Question: {question}\n{custom_instruction}"},
        ]
        continue_final_message = True

        if("begin_think" in model_config):
            if(self.model_name == "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"):
                history.append({"role": "assistant", "content": "<think>"})
            else:
                continue_final_message = False
        elif("fuzzy_separator" in model_config):
            continue_final_message = False
        else:
            print(f"ERROR: model {self.model_name} missing CoT separator config")
            exit(1)

        prompt = self.tokenizer.apply_chat_template(history,
            tokenize=False,
            add_generation_prompt=not continue_final_message,
            continue_final_message=continue_final_message)
        return prompt

    def do_generate(self, question_id, prompt, max_new_tokens=4096):
        """Generate a response using Chain-of-Thought (CoT) prompting."""
        model_config = ModelConfig.get(self.model_name)

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        output = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.6,
            top_k=20,
            min_p=0.0,
            top_p=0.95,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.eos_token_id,
            output_scores=True,
            return_dict_in_generate=True,
        )
        return output

    def do_generate_batch(self, question_ids, prompts, max_new_tokens=4096):
        """Generate responses for multiple prompts in batch using Chain-of-Thought (CoT) prompting."""
        model_config = ModelConfig.get(self.model_name)

        # Validate inputs
        if not prompts:
            raise ValueError("Empty prompts list provided to do_generate_batch")
        
        if len(question_ids) != len(prompts):
            raise ValueError(f"Mismatch between question_ids ({len(question_ids)}) and prompts ({len(prompts)})")

        # Tokenize all prompts at once
        try:
            inputs = self.tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(self.model.device)
        except Exception as e:
            print(f"Batch tokenization failed: {e}")
            print(f"Number of prompts: {len(prompts)}")
            print(f"First prompt: {prompts[0] if prompts else 'None'}")
            print("Falling back to individual tokenization...")
            
            # Fallback: tokenize individually and combine
            input_ids = []
            attention_masks = []
            for prompt in prompts:
                tokenized = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                input_ids.append(tokenized["input_ids"])
                attention_masks.append(tokenized["attention_mask"])
            
            # Pad to same length
            max_length = max(ids.shape[1] for ids in input_ids)
            padded_input_ids = []
            padded_attention_masks = []
            
            for ids, mask in zip(input_ids, attention_masks):
                pad_length = max_length - ids.shape[1]
                if pad_length > 0:
                    padded_ids = torch.cat([ids, torch.full((1, pad_length), self.tokenizer.pad_token_id, dtype=ids.dtype)], dim=1)
                    padded_mask = torch.cat([mask, torch.zeros((1, pad_length), dtype=mask.dtype)], dim=1)
                else:
                    padded_ids = ids
                    padded_mask = mask
                padded_input_ids.append(padded_ids)
                padded_attention_masks.append(padded_mask)
            
            inputs = {
                "input_ids": torch.cat(padded_input_ids, dim=0).to(self.model.device),
                "attention_mask": torch.cat(padded_attention_masks, dim=0).to(self.model.device)
            }
        
        output = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.6,
            top_k=20,
            min_p=0.0,
            top_p=0.95,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.eos_token_id,
            output_scores=True,
            return_dict_in_generate=True,
        )
        return output

    def get_log_probs(self, sequences: torch.Tensor):
        with torch.no_grad():
            outputs = self.model(input_ids=sequences)
            log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)
        return log_probs

    def get_log_probs_batch(self, sequences_list: List[torch.Tensor]):
        """Get log probabilities for multiple sequences in batch."""
        with torch.no_grad():
            # Stack all sequences into a single batch
            # Pad sequences to the same length
            max_length = max(seq.shape[1] for seq in sequences_list)
            padded_sequences = []
            
            for seq in sequences_list:
                if seq.shape[1] < max_length:
                    # Pad with pad_token_id
                    pad_length = max_length - seq.shape[1]
                    padding = torch.full((seq.shape[0], pad_length), self.tokenizer.pad_token_id, 
                                       dtype=seq.dtype, device=seq.device)
                    padded_seq = torch.cat([seq, padding], dim=1)
                else:
                    padded_seq = seq
                padded_sequences.append(padded_seq)
            
            # Stack into batch
            batch_sequences = torch.cat(padded_sequences, dim=0)
            
            # Get log probabilities for the entire batch
            outputs = self.model(input_ids=batch_sequences)
            log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)
            
            # Split back into individual sequences
            log_probs_list = []
            start_idx = 0
            for seq in sequences_list:
                end_idx = start_idx + seq.shape[0]
                log_probs_list.append(log_probs[start_idx:end_idx])
                start_idx = end_idx
            
            return log_probs_list

    def do_split(self, sequences):
        model_config = ModelConfig.get(self.model_name)

        # should split the output into three parts: question, the chain of thought and the answer
        if("begin_think" in model_config):
            # Split before decoding
            begin_think = self._get_token_id(model_config["begin_think"])
            if(sequences[0][0] == begin_think):
                sequences[0] = sequences[0][1:]
            end_think = self._get_token_id(model_config["end_think"])
            # split to 3 pieces: piece 0: question; piece 1: cot; piece 2: answer
            pieces = self._split_on_tokens(sequences[0].tolist(), [begin_think, end_think])

            if len(pieces) < 3:
                full = self.tokenizer.decode(sequences[0], skip_special_tokens=True)
                raise RuntimeError(
                    f"Failed to extract CoT (too few pieces) from: {full}"
                )

            response0 = self.tokenizer.decode(pieces[0], skip_special_tokens=True)
            response1 = self.tokenizer.decode(pieces[1], skip_special_tokens=True)
            response2 = self.tokenizer.decode(pieces[2], skip_special_tokens=True)

            question = response0.strip()
            #cot = response0[len(prompt):].strip()
            cot = response1.strip()
            answer = response2.strip()

        elif("fuzzy_separator" in model_config):
            if(model_config["fuzzy_separator"] in sequences):
                pieces = sequences.split(model_config["fuzzy_separator"])
            else:
                print(f"ERROR: model {self.model_name} did not generate chain of thought separator {model_config['fuzzy_separator']}")
                # print(f"Response: {full_response}")
                exit(1)
            #cot = response1.strip()
            #answer = response2.strip()

        else:
            raise RuntimeError(f"Model {self.model_name} missing CoT separator config")


        return (question, cot, answer)

    def generate_cot_response_full(self, question_id, question, max_new_tokens=4096):
        """Generate a response using Chain-of-Thought (CoT) prompting."""
        prompt = self.make_prompt(question_id, question)
        output = self.do_generate(question_id, prompt, max_new_tokens)
        sequences = output.sequences

        raw_output = self.tokenizer.decode(sequences[0], skip_special_tokens=True)

        (question, cot, answer) = self.do_split(sequences)

        return ModelResponse(
            question_id=question_id,
            question=question,
            prompt=prompt,
            cot=cot,
            answer=answer,
            raw_output=raw_output)

    def generate_cot_response_full_batch(self, question_ids, questions, max_new_tokens=4096):
        """Generate responses for multiple questions in batch using Chain-of-Thought (CoT) prompting."""
        # Validate inputs
        if not question_ids or not questions:
            raise ValueError("Empty question_ids or questions list provided")
        
        if len(question_ids) != len(questions):
            raise ValueError(f"Mismatch between question_ids ({len(question_ids)}) and questions ({len(questions)})")
        
        # Create prompts for all questions
        prompts = []
        for qid, question in zip(question_ids, questions):
            if not question or question.strip() == "":
                raise ValueError(f"Empty question for question_id {qid}")
            prompt = self.make_prompt(qid, question)
            if not prompt or prompt.strip() == "":
                raise ValueError(f"Empty prompt generated for question_id {qid}")
            prompts.append(prompt)
        
        # Generate responses in batch
        output = self.do_generate_batch(question_ids, prompts, max_new_tokens)
        sequences = output.sequences

        # Process each response
        responses = []
        for i, (question_id, question, prompt) in enumerate(zip(question_ids, questions, prompts)):
            raw_output = self.tokenizer.decode(sequences[i], skip_special_tokens=True)
            
            try:
                (question_part, cot, answer) = self.do_split(sequences[i:i+1])
                
                response = ModelResponse(
                    question_id=question_id,
                    question=question,
                    prompt=prompt,
                    cot=cot,
                    answer=answer,
                    raw_output=raw_output
                )
                responses.append(response)
            except RuntimeError as e:
                # Handle cases where splitting fails
                print(f"Warning: Failed to split response for question {question_id}: {e}")
                response = ModelResponse(
                    question_id=question_id,
                    question=question,
                    prompt=prompt,
                    cot="",
                    answer=raw_output,
                    raw_output=raw_output
                )
                responses.append(response)

        return responses

    def evaluate_cot_response(self, question_id, prompt, max_new_tokens=4096):
        """Generate a response using Chain-of-Thought (CoT) prompting."""
        prompt_tokens = self.utils.encode_to_tensor(prompt)
        return self.evaluate_cot_response_from_tokens(question_id, prompt_tokens, max_new_tokens)

    def evaluate_cot_response_from_tokens(self, question_id, prompt_tokens: torch.Tensor, max_new_tokens=4096):
        log_probs = self.get_log_probs(prompt_tokens)

        raw_output = self.tokenizer.decode(prompt_tokens, skip_special_tokens=True)

        (question, cot, answer) = self.do_split(log_probs, raw_output, prompt_tokens)

        return ModelResponse(
            question_id=question_id,
            question=question,
            prompt=prompt_tokens,
            cot=cot,
            answer=answer,
            raw_output=raw_output)

    def _split_on_tokens(self, lst, token_list):
        """Split a list into sublists, using 'token' as the delimiter (token is not included in results)."""
        result = []
        current = []
        for item in lst:
            if item in token_list:
                result.append(current)
                current = []
            else:
                current.append(item)
        result.append(current)
        return result

    def _get_token_id(self, token):
        token_id = self.tokenizer.convert_tokens_to_ids(token)
        if(token_id is None):
            print(f"ERROR: model {self.model_name} does not support {token} token")
            exit(1)
        return token_id

    def get_think_tokens(self):
        model_config = ModelConfig.get(self.model_name)

        begin_think = self._get_token_id(model_config["begin_think"])
        end_think = self._get_token_id(model_config["end_think"])
        return (begin_think, end_think)

if __name__ == "__main__":
    question = "A car travels 60 miles in 1.5 hours. What is its average speed?"
    print("Prompt: " + question.encode('unicode_escape').decode())

    model = Model("deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", cache_dir="/tmp/cache2")
    (cot, answer) = model.generate_cot_response(0, question)
    print("\n")
    print("CoT: " + cot.encode('unicode_escape').decode())
    print("\n")
    print("Answer: " + answer.encode('unicode_escape').decode())
    print("\n")
    print("Evaluate replacing CoT with thinking TOKENS:")
    print("\n")
    model.evaluate_with_custom_cot_tokens(question,
        model.tokenizer.encode(cot, return_tensors="pt").to(model.model.device).squeeze(0))

