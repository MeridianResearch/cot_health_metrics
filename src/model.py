from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import AutoConfig
from transformers import StoppingCriteria, StoppingCriteriaList
import torch
from dataclasses import dataclass
from token_utils import TokenUtils
from model_prompts import ModelPromptBuilder
from typing import Optional, List, Callable
from config import ModelConfig
from model_factory import ModelComponentFactory
from peft import PeftModel
import os

class _EndsWithCriteria(StoppingCriteria):
    def __init__(self, tokenizer, stop_strings):
        self.stop_ids = [tokenizer.encode(s, add_special_tokens=False) for s in stop_strings]

    def __call__(self, input_ids, scores, **kwargs) -> bool:
        ids = input_ids[0].tolist()
        for sid in self.stop_ids:
            n = len(sid)
            if n and len(ids) >= n and ids[-n:] == sid:
                return True
        return False

class Model:
    def __init__(self, model_name: str, cache_dir="/tmp/cache"):
        self.model_name = model_name
        self.cache_dir = cache_dir

    def get_utils(self):
        raise NotImplementedError("Subclasses must implement this method")

    def make_prompt(self, question_id, question, custom_instruction=None):
        raise NotImplementedError("Subclasses must implement this method")

    def do_generate(self, question_id, prompt, max_new_tokens=4096):
        raise NotImplementedError("Subclasses must implement this method")

    def generate_cot_response(self, question_id, question, max_new_tokens=4096):
        raise NotImplementedError("Subclasses must implement this method")

    def evaluate_cot_response(self, question_id, prompt, max_new_tokens=4096):
        raise NotImplementedError("Subclasses must implement this method")

    def get_log_probs(self, sequences: torch.Tensor):
        raise NotImplementedError("Subclasses must implement this method")

    def do_split(self, sequences):
        raise NotImplementedError("Subclasses must implement this method")


class Model:
    def __init__(self, model_name: str, cache_dir="/tmp/cache"):
        self.model_name = model_name
        self.cache_dir = cache_dir

    def get_utils(self):
        raise NotImplementedError("Subclasses must implement this method")

    def make_prompt(self, question_id, question, custom_instruction=None):
        raise NotImplementedError("Subclasses must implement this method")

    def do_generate(self, question_id, prompt, max_new_tokens=4096):
        raise NotImplementedError("Subclasses must implement this method")

    def generate_cot_response(self, question_id, question, max_new_tokens=4096):
        raise NotImplementedError("Subclasses must implement this method")

    def evaluate_cot_response(self, question_id, prompt, max_new_tokens=4096):
        raise NotImplementedError("Subclasses must implement this method")

    def get_log_probs(self, sequences: torch.Tensor):
        raise NotImplementedError("Subclasses must implement this method")

    def do_split(self, sequences):
        raise NotImplementedError("Subclasses must implement this method")


@dataclass
class ModelResponse:
    """
    question: original question
    prompt: Question: {question}+{custom_instruction}+<think>
    cot: chain of thought
    answer: answer generated by the model
    """
    question_id: Optional[str]
    question: str
    prompt: str
    cot: str
    answer: str
    raw_output: str

    def __post_init__(self):
        self.basic_pair = (self.cot, self.answer)

    def old__str__(self):
        return f"""
ModelResponse(
    Question: {self.question},
    Prompt: {self._encode(self.prompt)}
    CoT: {self._encode(self.cot)}
    Answer: {self._encode(self.answer)}
)
"""

    def print(self):
        print(f"Question: {self._encode(self.question)}")
        print("\n")
        print(f"Prompt: {self._encode(self.prompt)}")
        print("\n")
        print("CoT: " + self._encode(self.cot))
        print("\n")
        print(f"Answer: {self._encode(self.answer)}")
        print("\n")

class CoTModel(Model):
    def __init__(self, model_name: str,
        component_factory: ModelComponentFactory = None,
        cache_dir="/tmp/cache",
        adapter_path: str | None = None,
        system_prompt: str | None = None):

        super().__init__(model_name, cache_dir)

        self.system_prompt = system_prompt

        self.tokenizer, self.model = self._load_model(model_name, cache_dir, adapter_path)

        self.component_factory = component_factory or ModelComponentFactory(model_name)

        if not ModelConfig.is_supported(model_name):
            print(f"ERROR: model {model_name} is not in supported list {ModelConfig.SUPPORTED_MODELS}")
            exit(1)

        self.utils = TokenUtils(self.model, self.tokenizer)

    def _load_model(self, model_name, cache_dir, adapter_path=None):
        config = AutoConfig.from_pretrained(
            model_name, cache_dir=cache_dir, trust_remote_code=True
        )

        # tokenizer from adapter if existing (to get added_tokens + chat template)
        tok_src = adapter_path if (adapter_path and os.path.exists(os.path.join(adapter_path, "tokenizer.json"))) else model_name
        tokenizer = AutoTokenizer.from_pretrained(tok_src, cache_dir=cache_dir)

        # base
        base = AutoModelForCausalLM.from_pretrained(
            model_name,
            config=config,
            dtype=torch.float16 if "Qwen" in model_name else None,  # avoids the deprecation warning
            device_map="auto",
            cache_dir=cache_dir,
            trust_remote_code=True,
        )

        # so embeddings match tokenizer size (for added tokens)
        base.resize_token_embeddings(len(tokenizer))

        # apply LoRA
        if adapter_path:
            base = PeftModel.from_pretrained(base, adapter_path)

        # pad token QoL
        if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:
            tokenizer.pad_token = tokenizer.eos_token

        # load chat template if existing with adapter
        chat_tmpl_file = os.path.join(tok_src, "chat_template.jinja")
        if os.path.exists(chat_tmpl_file):
            try:
                with open(chat_tmpl_file, "r", encoding="utf-8") as f:
                    tokenizer.chat_template = f.read()
            except Exception:
                pass

        return (tokenizer, base)

    def get_utils(self):
        return self.utils

    def generate_cot_response(self, question_id, question, max_new_tokens=4096, do_sample=True):
        final_response = self.generate_cot_response_full(question_id, question,
            max_new_tokens=max_new_tokens, do_sample=do_sample)
        return final_response.basic_pair

    def generate_cot_response_batch(self, question_ids, questions, max_new_tokens=4096):
        """Generate chain-of-thought responses for multiple questions in batch."""
        responses = self.generate_cot_response_full_batch(question_ids, questions, max_new_tokens)
        return [response.basic_pair for response in responses]

    def make_prompt(self, question_id, question, custom_instruction=None):
        prompt_builder = self.component_factory.make_prompt_builder(invokes_cot=True)
        prompt_builder.add_user_message(question, custom_instruction or self.system_prompt)
        return prompt_builder.make_prompt(self.tokenizer)

    def make_prompt_no_cot(self, question_id, question):
        prompt_builder = self.component_factory.make_prompt_builder(invokes_cot=False)
        prompt_builder.add_user_message(question)
        return prompt_builder.make_prompt(self.tokenizer)

    def do_generate(self, question_id, prompt, max_new_tokens=4096, do_sample=True):
        """Generate a response using Chain-of-Thought (CoT) prompting."""
        model_config = ModelConfig.get(self.model_name)

        generate_kwargs = model_config.get("generate_kwargs", {})

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        output = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.eos_token_id,
            output_scores=True,
            return_dict_in_generate=True,
            **generate_kwargs,
        )
        return output

    def _generate_with_stop(self, prompt, *, max_new_tokens, do_sample, stop_strings,
                            no_repeat_ngram_size=3, repetition_penalty=1.0):
        model_config    = ModelConfig.get(self.model_name)
        generate_kwargs = dict(model_config.get("generate_kwargs", {}))
        generate_kwargs.update(
            dict(
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id,
                output_scores=True,
                return_dict_in_generate=True,
                no_repeat_ngram_size=no_repeat_ngram_size,
                repetition_penalty=repetition_penalty,
                stopping_criteria=StoppingCriteriaList([_EndsWithCriteria(self.tokenizer, stop_strings)]),
            )
        )
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            out = self.model.generate(**inputs, **generate_kwargs)
        prompt_len = inputs["input_ids"].shape[1]
        cont_ids   = out.sequences[0, prompt_len:]
        return self.tokenizer.decode(cont_ids, skip_special_tokens=True), out

    def do_generate_batch(self, question_ids, prompts, max_new_tokens=4096, do_sample=True):
        """Generate responses for multiple prompts in batch using Chain-of-Thought (CoT) prompting."""
        model_config = ModelConfig.get(self.model_name)

        # Validate inputs
        if not prompts:
            raise ValueError("Empty prompts list provided to do_generate_batch")
        
        if len(question_ids) != len(prompts):
            raise ValueError(f"Mismatch between question_ids ({len(question_ids)}) and prompts ({len(prompts)})")

        # Tokenize all prompts at once
        try:
            inputs = self.tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(self.model.device)
        except Exception as e:
            print(f"Batch tokenization failed: {e}")
            print(f"Number of prompts: {len(prompts)}")
            print(f"First prompt: {prompts[0] if prompts else 'None'}")
            print("Falling back to individual tokenization...")
            
            # Fallback: tokenize individually and combine
            input_ids = []
            attention_masks = []
            for prompt in prompts:
                tokenized = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                input_ids.append(tokenized["input_ids"])
                attention_masks.append(tokenized["attention_mask"])
            
            # Pad to same length
            max_length = max(ids.shape[1] for ids in input_ids)
            padded_input_ids = []
            padded_attention_masks = []
            
            for ids, mask in zip(input_ids, attention_masks):
                pad_length = max_length - ids.shape[1]
                if pad_length > 0:
                    padded_ids = torch.cat([ids, torch.full((1, pad_length), self.tokenizer.pad_token_id, dtype=ids.dtype)], dim=1)
                    padded_mask = torch.cat([mask, torch.zeros((1, pad_length), dtype=mask.dtype)], dim=1)
                else:
                    padded_ids = ids
                    padded_mask = mask
                padded_input_ids.append(padded_ids)
                padded_attention_masks.append(padded_mask)
            
            inputs = {
                "input_ids": torch.cat(padded_input_ids, dim=0).to(self.model.device),
                "attention_mask": torch.cat(padded_attention_masks, dim=0).to(self.model.device)
            }

        generate_kwargs = model_config.get("generate_kwargs", {})
        
        output = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.eos_token_id,
            output_scores=True,
            return_dict_in_generate=True,
            **generate_kwargs,
        )
        return output

    def get_log_probs(self, sequences: torch.Tensor):
        with torch.no_grad():
            outputs = self.model(input_ids=sequences)
            log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)
        return log_probs

    def get_log_probs_batch(self, sequences_list: List[torch.Tensor]):
        """Get log probabilities for multiple sequences in batch."""
        with torch.no_grad():
            # Stack all sequences into a single batch
            # Pad sequences to the same length
            max_length = max(seq.shape[1] for seq in sequences_list)
            padded_sequences = []
            
            for seq in sequences_list:
                if seq.shape[1] < max_length:
                    # Pad with pad_token_id
                    pad_length = max_length - seq.shape[1]
                    padding = torch.full((seq.shape[0], pad_length), self.tokenizer.pad_token_id, 
                                       dtype=seq.dtype, device=seq.device)
                    padded_seq = torch.cat([seq, padding], dim=1)
                else:
                    padded_seq = seq
                padded_sequences.append(padded_seq)
            
            # Stack into batch
            batch_sequences = torch.cat(padded_sequences, dim=0)
            
            # Get log probabilities for the entire batch
            outputs = self.model(input_ids=batch_sequences)
            log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)
            
            # Split back into individual sequences
            log_probs_list = []
            start_idx = 0
            for seq in sequences_list:
                end_idx = start_idx + seq.shape[0]
                log_probs_list.append(log_probs[start_idx:end_idx])
                start_idx = end_idx
            
            return log_probs_list

    def do_split(self, sequences, prompt):
        model_config = ModelConfig.get(self.model_name)

        # should split the output into three parts: question, the chain of thought and the answer
        if("begin_think" in model_config):
            begin_think = model_config["begin_think"]
            end_think = model_config["end_think"]

            full = self.tokenizer.decode(sequences[0], skip_special_tokens=False)

            try:
                (question, cot_and_answer) = full.split(begin_think, 1)
                (cot, answer) = cot_and_answer.split(end_think, 1)
            except ValueError:
                raise RuntimeError(
                    f"Failed to extract CoT (no begin/end think token) from: {full}"
                )

            question = question.strip()
            cot = cot.strip()
            answer = answer.strip()

        elif("fuzzy_end_think_list" in model_config):
            input_tokens = self.tokenizer(prompt, return_tensors="pt")
            full = self.tokenizer.decode(sequences[0], skip_special_tokens=False)
            question = self.tokenizer.decode(
                sequences[0][:len(input_tokens.input_ids[0])], skip_special_tokens=True).strip()
            cot_and_answer = self.tokenizer.decode(
                sequences[0][len(input_tokens.input_ids[0]):], skip_special_tokens=True)

            end_think_list = model_config["fuzzy_end_think_list"]
            for end_think in end_think_list:
                pieces = cot_and_answer.split(end_think, 1)
                if len(pieces) == 2:
                    cot = pieces[0].strip()
                    answer = pieces[1].strip()
                    break
            else:
                raise RuntimeError(
                    f"Failed to extract CoT (no end think token in {end_think_list}) from: {full}"
                    f"Model {self.model_name} did not generate known fuzzy split sequence in "
                    f"{model_config['fuzzy_end_think_list']}"
                )


        return (question, cot, answer)

    def generate_cot_response_full(self, question_id, question, max_new_tokens=4096, do_sample=False):
        """Two-stage generation with enforced <think>...</think> and a short Answer: line."""
        prompt_base = self.make_prompt(question_id, question)

        cfg         = ModelConfig.get(self.model_name)
        think_open  = cfg.get("begin_think", "<think>")
        think_close = cfg.get("end_think", "</think>")
        max_think   = cfg.get("max_new_tokens_think", 160)
        max_ans     = cfg.get("max_new_tokens_answer", 8)

        # CoT until </think>
        prompt1 = prompt_base + f"{think_open}\n"
        gen1_txt, _ = self._generate_with_stop(
            prompt1,
            max_new_tokens=max_think,
            do_sample=do_sample,
            stop_strings=[think_close],
            no_repeat_ngram_size=3,
            repetition_penalty=1.0,
        )
        cot = gen1_txt.rstrip()
        if cot.endswith(think_close):
            cot = cot[:-len(think_close)].rstrip()

        # short Answer: line (stop at newline/chat end)
        answer_prefix = "Answer:"
        think_block   = f"{think_open}\n{cot}\n{think_close}"
        prompt2       = prompt_base + f"{think_block}\n{answer_prefix} "

        gen2_txt, _ = self._generate_with_stop(
            prompt2,
            max_new_tokens=max_ans,
            do_sample=False,
            stop_strings=["\n", "<|im_end|>", "<|im_start|>"],
            no_repeat_ngram_size=3,
            repetition_penalty=1.0,
        )
        answer = gen2_txt.strip().splitlines()[0].strip().strip(" '\"")

        raw_output = f"{think_block}\n{answer_prefix} {answer}"
        return ModelResponse(
            question_id=question_id,
            question=question,
            prompt=prompt_base,
            cot=cot,
            answer=answer,
            raw_output=raw_output,
        )

    def evaluate_cot_response(self, question_id, prompt, response, max_new_tokens=4096, to_device=None):
        """Generate a response using Chain-of-Thought (CoT) prompting."""
        response_tokens = self.utils.encode_to_tensor(response, to_device=to_device)

        (question, cot, answer) = self.do_split(response_tokens, prompt)

        return ModelResponse(
            question_id=question_id,
            question=question,  # NOTE: this question is not fully parsed, contains make_prompt stuff
            prompt=prompt,
            cot=cot,
            answer=answer,
            raw_output=response)

    def generate_cot_response_full_batch(self, question_ids, questions, max_new_tokens=4096, do_sample=False):
        """Two-stage batched generation: Stage-1 (batched) for CoT, Stage-2 (per-item) for clean Answer: line."""
        if not question_ids or not questions:
            raise ValueError("Empty question_ids or questions list provided")
        if len(question_ids) != len(questions):
            raise ValueError(f"Mismatch between question_ids ({len(question_ids)}) and questions ({len(questions)})")

        prompts = [self.make_prompt(qid, q) for qid, q in zip(question_ids, questions)]

        cfg         = ModelConfig.get(self.model_name)
        think_open  = cfg.get("begin_think", "<think>")
        think_close = cfg.get("end_think", "</think>")
        max_think   = cfg.get("max_new_tokens_think", 160)
        max_ans     = cfg.get("max_new_tokens_answer", 8)

        # batched CoT
        prompts1 = [p + f"{think_open}\n" for p in prompts]
        inputs1  = self.tokenizer(prompts1, return_tensors="pt", padding=True).to(self.model.device)

        generate_kwargs = dict(cfg.get("generate_kwargs", {}))
        generate_kwargs.update(
            dict(
                max_new_tokens=max_think,
                do_sample=do_sample,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id,
                output_scores=True,
                return_dict_in_generate=True,
                no_repeat_ngram_size=3,
                repetition_penalty=1.0,
                stopping_criteria=StoppingCriteriaList([_EndsWithCriteria(self.tokenizer, [think_close])]),
            )
        )

        with torch.no_grad():
            out1 = self.model.generate(**inputs1, **generate_kwargs)

        # decode CoTs row-wise (continuations only)
        cots = []
        for i in range(out1.sequences.shape[0]):
            prompt_len = int(inputs1["attention_mask"][i].sum())
            cont_ids   = out1.sequences[i, prompt_len:]
            t          = self.tokenizer.decode(cont_ids, skip_special_tokens=True).rstrip()
            if t.endswith(think_close):
                t = t[:-len(think_close)].rstrip()
            cots.append(t)

        # per-item Answer:
        answer_prefix = "Answer:"
        responses = []
        for (qid, q, p, cot) in zip(question_ids, questions, prompts, cots):
            think_block = f"{think_open}\n{cot}\n{think_close}"
            p2          = p + f"{think_block}\n{answer_prefix} "

            a_txt, _ = self._generate_with_stop(
                p2,
                max_new_tokens=max_ans,
                do_sample=False,
                stop_strings=["\n", "<|im_end|>", "<|im_start|>"],
                no_repeat_ngram_size=3,
                repetition_penalty=1.0,
            )
            ans = a_txt.strip().splitlines()[0].strip().strip(" '\"")
            raw = f"{think_block}\n{answer_prefix} {ans}"

            responses.append(ModelResponse(qid, q, p, cot, ans, raw))
        return responses

    def _split_on_tokens(self, lst, token_list):
        """Split a list into sublists, using 'token' as the delimiter (token is not included in results)."""
        result = []
        current = []
        for item in lst:
            if item in token_list:
                result.append(current)
                current = []
            else:
                current.append(item)
        result.append(current)
        return result

    def _get_token_id(self, token):
        token_id = self.tokenizer.convert_tokens_to_ids(token)
        if(token_id is None):
            print(f"ERROR: model {self.model_name} does not support {token} token")
            exit(1)
        return token_id

    def get_think_tokens(self):
        model_config = ModelConfig.get(self.model_name)

        # Tokenize the begin_think and end_think strings to get all token IDs
        begin_think_text = model_config["begin_think"]
        end_think_text = model_config["end_think"]

        # Encode to get token IDs (returns tensor, so convert to list)
        begin_think_tokens = self.tokenizer.encode(begin_think_text, add_special_tokens=True)
        end_think_tokens = self.tokenizer.encode(end_think_text, add_special_tokens=True)

        # Convert to lists if they're tensors
        if hasattr(begin_think_tokens, 'tolist'):
            begin_think_tokens = begin_think_tokens.tolist()
        if hasattr(end_think_tokens, 'tolist'):
            end_think_tokens = end_think_tokens.tolist()

        return (begin_think_tokens, end_think_tokens)

if __name__ == "__main__":
    question = "How can one decide the best time to buy a house in Boston?"
    print("Prompt: " + question.encode('unicode_escape').decode())

    model = CoTModel("Qwen/Qwen3-1.7B", cache_dir="/tmp/cache2")
    (cot, answer) = model.generate_cot_response(1, question)
    print("\n")
    print("CoT: " + cot.encode('unicode_escape').decode())
    print("\n")
    print("Answer: " + answer.encode('unicode_escape').decode())
    print("\n")