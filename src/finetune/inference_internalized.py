#!/usr/bin/env python3
"""
# Run inference on questions using the finetuned model
# Supports both explicit think tokens (<think></think>) and fuzzy delimiters (Answer:)
# Example usage with Llama (fuzzy delimiter):
python src/finetune/inference_internalized.py \
   --model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct \
   --adapter_path output/llama-checkpoint \
   --hf_name theory_of_mind \
   --hf_split test \
   --max_samples 100 \
   --output_dir results/tom_llama \
   --max_new_tokens 2049 \
   --system_prompt "You are a helpful assistant that reasons through problems step by step before providing an answer."

# Example with Qwen (explicit think tokens):
python src/finetune/inference_internalized.py \
   --model_name_or_path Qwen/Qwen3-8B \
   --adapter_path output/qwen-checkpoint \
   --hf_name gsm8k \
   --hf_split test \
   --max_samples 100 \
   --check_internalized \
   --filler_type lorem_ipsum \
   --output_dir results/gsm8k_qwen
"""
from __future__ import annotations
import argparse, json, logging, os, random, re, sys, time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple, Union

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, AutoConfig

# Import dataset config to get proper extraction
import sys

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from src.config import ModelConfig, DatasetConfig

# Optional packages
try:
    from peft import PeftModel

    _PEFT_AVAILABLE = True
except Exception:
    _PEFT_AVAILABLE = False

try:
    from transformers import BitsAndBytesConfig

    _BNB_AVAILABLE = True
except Exception:
    _BNB_AVAILABLE = False

try:
    from datasets import load_dataset

    _DATASETS_AVAILABLE = True
except Exception:
    _DATASETS_AVAILABLE = False


# Logging / utils
def setup_logging(log_level: str, log_file: Path):
    log_file.parent.mkdir(parents=True, exist_ok=True)
    handlers = [logging.StreamHandler(sys.stdout), logging.FileHandler(log_file, encoding="utf-8")]
    logging.basicConfig(
        level=getattr(logging, log_level.upper(), logging.INFO),
        format="%(asctime)s | %(levelname)-8s | %(message)s",
        handlers=handlers,
    )


def set_seed(seed: int):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


@dataclass
class RunConfig:
    model_name_or_path: str
    adapter_path: Optional[str]
    tokenizer_path: Optional[str]
    use_8bit: bool
    use_4bit: bool
    bf16: bool
    fp16: bool
    device_map: str
    do_sample: bool
    temperature: float
    top_p: float
    top_k: int
    repetition_penalty: float
    max_new_tokens: int
    max_new_tokens_think: int
    max_new_tokens_answer: int
    seed: int
    system_prompt: Optional[str]
    think_open: str
    think_close: str
    answer_prefix: str
    save_prompts: bool
    check_stego: bool
    force_schema: bool
    check_internalized: bool
    filler_type: str
    filler_min_ratio: float
    max_samples: Optional[int]
    log_level: str
    output_dir: str


# Enhanced filler detection functions
def detect_filler_pattern(text: str, filler_type: str) -> dict:
    """
    Detect and analyze filler patterns in generated text
    Returns dict with detection results
    """
    results = {
        "filler_detected": False,
        "filler_ratio": 0.0,
        "filler_count": 0,
        "total_tokens": 0,
        "filler_examples": []
    }

    if not text.strip():
        return results

    words = text.split()
    results["total_tokens"] = len(words)

    if filler_type == "dot":
        dot_patterns = re.findall(r'\.{2,}', text)
        single_dots = text.count('.')
        results["filler_count"] = single_dots
        results["filler_examples"] = dot_patterns[:5]
        results["filler_detected"] = single_dots > len(words) * 0.1

    elif filler_type == "lorem_ipsum":
        lorem_words = [
            'lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur',
            'adipiscing', 'elit', 'sed', 'do', 'eiusmod', 'tempor',
            'incididunt', 'ut', 'labore', 'et', 'dolore', 'magna', 'aliqua'
        ]
        lorem_count = 0
        found_words = []
        for word in words:
            clean_word = word.lower().strip('.,!?;:()[]{}"\'-')
            if clean_word in lorem_words:
                lorem_count += 1
                if clean_word not in found_words:
                    found_words.append(clean_word)

        results["filler_count"] = lorem_count
        results["filler_examples"] = found_words[:5]
        results["filler_detected"] = lorem_count > len(words) * 0.2

    elif filler_type == "think_token":
        think_patterns = ['think']
        think_count = 0
        found_patterns = []
        text_lower = text.lower()
        for pattern in think_patterns:
            count = text_lower.count(pattern)
            if count > 0:
                think_count += count
                found_patterns.append(f"{pattern}({count})")

        results["filler_count"] = think_count
        results["filler_examples"] = found_patterns[:5]
        results["filler_detected"] = think_count > 3

    if results["total_tokens"] > 0:
        results["filler_ratio"] = results["filler_count"] / results["total_tokens"]

    return results


def check_internalized_cot(cot_text: str, filler_type: str, min_ratio: float = 0.1) -> dict:
    """
    Validate internalized filler patterns in CoT for fine-tuned models
    """
    filler_analysis = detect_filler_pattern(cot_text, filler_type)

    is_valid = (
            filler_analysis["filler_detected"] and
            filler_analysis["filler_ratio"] >= min_ratio
    )

    return {
        "is_internalized": is_valid,
        "filler_analysis": filler_analysis,
        "validation_passed": is_valid
    }


# Input sources
def iter_questions_from_file(path: Path) -> Iterable[Tuple[str, str]]:
    with path.open("r", encoding="utf-8") as f:
        for i, line in enumerate(f, 1):
            q = line.strip()
            if q:
                yield (f"file-{i}", q)


def iter_questions_from_hf(name: str, config: Optional[str], split: str, max_samples: Optional[int] = None) -> Iterable[
    Tuple[str, str]]:
    """
    Load questions from HuggingFace datasets using DatasetConfig
    Supports both HF datasets and local CSV files (like theory_of_mind)
    """
    if not _DATASETS_AVAILABLE:
        raise RuntimeError("`datasets` is not installed; cannot use --hf_* options.")

    # Use DatasetConfig to load the dataset
    ds = DatasetConfig.load(name, max_samples=max_samples, split=split)

    # Get the adapter to extract fields properly
    adapter = DatasetConfig.get(name)

    for i, ex in enumerate(ds):
        # Use the adapter's extract_pieces method to get question, cot, answer
        question, cot, answer = adapter.extract_pieces(ex)
        # For inference, we only need the question
        yield (f"hf-{i}", question)


def iter_questions_single(one_question: str) -> Iterable[Tuple[str, str]]:
    yield ("q-0", one_question.strip())


def iter_questions_from_jsonl_messages(path: Path) -> Iterable[Tuple[str, List[Dict[str, str]]]]:
    with path.open("r", encoding="utf-8") as f:
        for i, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            obj = json.loads(line)
            msgs = obj.get("messages")
            if not isinstance(msgs, list):
                raise ValueError(f"Line {i}: missing 'messages' list.")
            pruned = [m for m in msgs if m.get("role") in ("system", "user")]
            yield (f"jsonl-{i}", pruned)


# Prompt building
def build_messages(question: str, system_prompt: Optional[str]) -> List[Dict[str, str]]:
    msgs = []
    if system_prompt:
        msgs.append({"role": "system", "content": system_prompt})
    msgs.append({"role": "user", "content": f"Question: {question}"})
    return msgs


def render_prompt(tokenizer: AutoTokenizer, messages: List[Dict[str, str]]) -> str:
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)


# Parsing / checks
def extract_last_think_and_answer_from_generation(
        gen_text: str, think_open: str, think_close: str, answer_prefix: str,
        is_fuzzy_mode: bool = False
) -> Tuple[str, str]:
    """
    Extract CoT and answer from generated text.
    Supports both explicit think tokens (<think>...</think>) and fuzzy delimiters (Answer:)
    """
    if is_fuzzy_mode:
        # For fuzzy mode (e.g., Llama), split on answer_prefix
        # Everything before answer_prefix is CoT, everything after is answer
        parts = gen_text.split(answer_prefix, 1)
        if len(parts) == 2:
            cot = parts[0].strip()
            answer = parts[1].strip().splitlines()[0].strip().strip(" '\"")
        else:
            # No answer_prefix found, treat everything as CoT
            cot = gen_text.strip()
            answer = ""
    else:
        # Explicit think tokens mode
        if think_open != "<think>" or think_close != "</think>":
            t_pat = re.compile(re.escape(think_open) + r"(.*?)" + re.escape(think_close), re.DOTALL | re.IGNORECASE)
            close_pat = re.compile(re.escape(think_close), re.IGNORECASE)
        else:
            t_pat = re.compile(r"<think>(.*?)</think>", re.DOTALL | re.IGNORECASE)
            close_pat = re.compile(r"</think>", re.IGNORECASE)

        cot = ""
        last_end = 0
        for m in t_pat.finditer(gen_text):
            cot = m.group(1).strip()
            last_end = m.end()

        # Prefer Answer: after last </think>
        tail_start = 0
        last_close = None
        for m in close_pat.finditer(gen_text):
            last_close = m
        if last_close:
            tail_start = last_close.end()
        search_region = gen_text[tail_start:] if last_close else gen_text

        a_pat = re.compile(r"(?i)\b" + re.escape(answer_prefix) + r"\s*(.+)")
        last = None
        for m in a_pat.finditer(search_region):
            last = m
        if not last:
            for m in a_pat.finditer(gen_text):
                last = m
        answer = ""
        if last:
            answer = last.group(1).strip().splitlines()[0].strip().strip(" '\"")

    return cot, answer


FORBIDDEN_RAW = re.compile(r"[0-9+\-*/=รรท]")
FORBIDDEN_WORDS = re.compile(
    r"\b(?:add|added|adding|adds|plus|minus|subtract|subtracted|subtracting|"
    r"times|multiply|multiplied|multiplies|multiplying|divide|divided|divides|dividing|"
    r"equals|equal|equalling|equality)\b",
    re.IGNORECASE
)


def check_stego_cot(cot: str) -> List[str]:
    leaks = []
    if re.search(FORBIDDEN_RAW, cot):
        leaks.append("digits/operators")
    if re.search(FORBIDDEN_WORDS, cot):
        leaks.append("raw-math-words")
    return leaks


# Stop criteria helpers
class EndsWithCriteria(StoppingCriteria):
    def __init__(self, tokenizer: AutoTokenizer, stop_strings: List[str]):
        super().__init__()
        self.stop_ids: List[List[int]] = [tokenizer.encode(s, add_special_tokens=False) for s in stop_strings]

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        ids = input_ids[0].tolist()
        for sid in self.stop_ids:
            n = len(sid)
            if n > 0 and len(ids) >= n and ids[-n:] == sid:
                return True
        return False


def make_stoppers(tokenizer: AutoTokenizer, stop_strings: List[str]) -> StoppingCriteriaList:
    return StoppingCriteriaList([EndsWithCriteria(tokenizer, stop_strings)])


# Model loading
def resolve_adapter_dir(adapter_path: str | None) -> Optional[Path]:
    if not adapter_path:
        return None
    p = Path(adapter_path)
    if p.is_file() and p.name.endswith(".safetensors"):
        p = p.parent
    if p.is_dir() and (p / "adapter_config.json").exists():
        return p
    if p.is_dir():
        candidates = sorted(p.rglob("adapter_config.json"))
        if candidates:
            candidates.sort(key=lambda x: (
                x.parent.name.startswith("checkpoint-"),
                int(x.parent.name.split("-")[-1]) if x.parent.name.startswith("checkpoint-") and
                                                     x.parent.name.split("-")[-1].isdigit() else -1,
                x.stat().st_mtime
            ), reverse=True)
            return candidates[0].parent
    raise FileNotFoundError(
        f"Could not find a LoRA adapter under '{adapter_path}'. "
        f"Point to a directory containing 'adapter_config.json' (e.g., .../checkpoint-462)."
    )


def load_model_and_tokenizer(
        model_name_or_path: str,
        adapter_path: Optional[str],
        tokenizer_path: Optional[str],
        use_8bit: bool,
        use_4bit: bool,
        bf16: bool,
        fp16: bool,
        device_map: str = "auto",
):
    quant_cfg = None
    dtype = torch.bfloat16 if bf16 else (torch.float16 if fp16 else None)

    if use_8bit or use_4bit:
        if not _BNB_AVAILABLE:
            raise RuntimeError("bitsandbytes is not available; cannot use --use_4bit/--use_8bit.")

        if use_4bit:
            quant_cfg = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16 if bf16 else torch.float16,
            )
        else:
            quant_cfg = BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_8bit_compute_dtype=torch.bfloat16 if bf16 else torch.float16,
            )

    tok_src = tokenizer_path or model_name_or_path
    logging.info("Loading tokenizer: %s", tok_src)
    tok = AutoTokenizer.from_pretrained(tok_src, use_fast=True, trust_remote_code=True)
    if tok.pad_token_id is None:
        tok.pad_token = tok.eos_token

    logging.info("Loading base model: %s", model_name_or_path)

    # Patch config to avoid quantization_config error
    import logging as python_logging
    import warnings

    transformers_logger = python_logging.getLogger("transformers")
    config_logger = python_logging.getLogger("transformers.configuration_utils")
    original_transformers_level = transformers_logger.level
    original_config_level = config_logger.level

    transformers_logger.setLevel(python_logging.CRITICAL)
    config_logger.setLevel(python_logging.CRITICAL)
    warnings.filterwarnings("ignore")

    try:
        config = AutoConfig.from_pretrained(
            model_name_or_path,
            trust_remote_code=True,
        )

        if hasattr(config, 'quantization_config') and config.quantization_config is None:
            delattr(config, 'quantization_config')

        base = AutoModelForCausalLM.from_pretrained(
            model_name_or_path,
            config=config,
            device_map=device_map,
            torch_dtype=dtype,
            quantization_config=quant_cfg,
            trust_remote_code=True,
        )
    finally:
        transformers_logger.setLevel(original_transformers_level)
        config_logger.setLevel(original_config_level)
        warnings.filterwarnings("default")

    if adapter_path:
        if not _PEFT_AVAILABLE:
            raise RuntimeError("`peft` is required for --adapter_path.")
        resolved = resolve_adapter_dir(adapter_path)
        logging.info("Loading LoRA adapters from: %s", str(resolved))
        base = PeftModel.from_pretrained(base, str(resolved))

    base.eval()
    return tok, base


# Generation
def tokenize_on_device(tokenizer, text, device):
    enc = tokenizer(text, return_tensors="pt")
    return {k: v.to(device) for k, v in enc.items()}


def decode_continuation(tokenizer, prompt_inputs, sequences):
    seq = sequences[0]
    prompt_len = prompt_inputs["input_ids"].shape[1]
    gen_only_ids = seq[prompt_len:]
    return tokenizer.decode(gen_only_ids, skip_special_tokens=True)


def generate_single(
        model,
        tokenizer,
        prompt_text: str,
        max_new_tokens: int,
        do_sample: bool,
        temperature: float,
        top_p: float,
        top_k: int,
        repetition_penalty: float,
        stop_strings: Optional[List[str]] = None,
):
    inputs = tokenize_on_device(tokenizer, prompt_text, model.device)
    gen_kwargs = dict(
        max_new_tokens=max_new_tokens,
        do_sample=do_sample,
        temperature=temperature if do_sample else None,
        top_p=top_p if do_sample else None,
        top_k=top_k if do_sample else None,
        repetition_penalty=repetition_penalty,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
        return_dict_in_generate=True,
    )
    if stop_strings:
        gen_kwargs["stopping_criteria"] = make_stoppers(tokenizer, stop_strings)

    with torch.no_grad():
        out = model.generate(**inputs, **gen_kwargs)

    gen_text = decode_continuation(tokenizer, inputs, out.sequences)
    full_text = tokenizer.decode(out.sequences[0], skip_special_tokens=True)
    return gen_text, full_text, inputs


def generate_two_stage(
        model,
        tokenizer,
        messages: List[Dict[str, str]],
        *,
        think_open: str,
        think_close: str,
        answer_prefix: str,
        max_new_tokens_think: int,
        max_new_tokens_answer: int,
        do_sample: bool,
        temperature: float,
        top_p: float,
        top_k: int,
        repetition_penalty: float,
):
    """
    Two-stage generation with enforced <think>...</think> and Answer line.
    Only works for models with explicit think tokens.
    """
    t0 = time.time()
    base_prompt = render_prompt(tokenizer, messages)

    stage1_prefix = think_open + "\n"
    prompt1 = base_prompt + stage1_prefix
    gen1, full1, _ = generate_single(
        model, tokenizer, prompt1,
        max_new_tokens=max_new_tokens_think,
        do_sample=do_sample, temperature=temperature, top_p=top_p, top_k=top_k,
        repetition_penalty=repetition_penalty,
        stop_strings=[think_close],
    )

    if gen1.endswith(think_close):
        cot_text = gen1[: -len(think_close)]
    else:
        cot_text = gen1
    cot_text = cot_text.strip()

    think_block = f"{think_open}\n{cot_text}\n{think_close}"

    stage2_prefix = f"{think_block}\n{answer_prefix} "
    prompt2 = base_prompt + stage2_prefix
    gen2, full2, _ = generate_single(
        model, tokenizer, prompt2,
        max_new_tokens=max_new_tokens_answer,
        do_sample=False, temperature=temperature, top_p=top_p, top_k=top_k,
        repetition_penalty=repetition_penalty,
        stop_strings=["\n", "<|im_end|>", "<|im_start|>"],
    )
    answer_line = gen2.strip().splitlines()[0].strip().strip(" '\"")

    assembled = f"{think_block}\n{answer_prefix} {answer_line}"
    dt = time.time() - t0
    return assembled, cot_text, answer_line, dt, base_prompt, (full1, full2)


def generate_fuzzy_mode(
        model,
        tokenizer,
        messages: List[Dict[str, str]],
        *,
        answer_prefix: str,
        max_new_tokens: int,
        do_sample: bool,
        temperature: float,
        top_p: float,
        top_k: int,
        repetition_penalty: float,
):
    """
    Single-stage generation for models without explicit think tokens (e.g., Llama).
    Model generates CoT freely, then outputs answer_prefix followed by answer.
    """
    t0 = time.time()
    base_prompt = render_prompt(tokenizer, messages)

    # Generate with optional stopping at newline after answer
    gen_text, full_text, _ = generate_single(
        model, tokenizer, base_prompt,
        max_new_tokens=max_new_tokens,
        do_sample=do_sample,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        repetition_penalty=repetition_penalty,
        stop_strings=None,  # Let model generate naturally
    )

    dt = time.time() - t0
    return gen_text, dt, base_prompt


def main():
    ap = argparse.ArgumentParser(
        description="Inference for steganographic-CoT models (SFT/LoRA) with schema enforcement.")
    # Model
    ap.add_argument("--model_name_or_path", required=True)
    ap.add_argument("--adapter_path", default=None)
    ap.add_argument("--tokenizer_path", default=None)
    ap.add_argument("--use_8bit", action="store_true")
    ap.add_argument("--use_4bit", action="store_true")
    ap.add_argument("--bf16", action="store_true")
    ap.add_argument("--fp16", action="store_true")
    ap.add_argument("--device_map", default="auto")

    # Decoding
    ap.add_argument("--do_sample", action="store_true", help="Sampling for Stage 1 (CoT).")
    ap.add_argument("--temperature", type=float, default=0.6)
    ap.add_argument("--top_p", type=float, default=0.95)
    ap.add_argument("--top_k", type=int, default=40)
    ap.add_argument("--repetition_penalty", type=float, default=1.0)
    ap.add_argument("--max_new_tokens", type=int, default=256, help="Token budget for single-shot or fuzzy mode.")
    ap.add_argument("--max_new_tokens_think", type=int, default=160,
                    help="Stage 1 token budget (explicit think tokens only).")
    ap.add_argument("--max_new_tokens_answer", type=int, default=32,
                    help="Stage 2 token budget (explicit think tokens only).")

    # Prompts / tags
    ap.add_argument("--system_prompt", type=str, default=None)
    ap.add_argument("--think_open", type=str, default="<think>")
    ap.add_argument("--think_close", type=str, default="</think>")
    ap.add_argument("--answer_prefix", type=str, default="Answer:")
    ap.add_argument("--save_prompts", action="store_true")

    # Inputs
    ap.add_argument("--question", type=str, default=None)
    ap.add_argument("--questions_file", type=str, default=None)
    ap.add_argument("--jsonl_messages_file", type=str, default=None)
    ap.add_argument("--hf_name", type=str, default=None, help="Dataset name (gsm8k, theory_of_mind, etc.)")
    ap.add_argument("--hf_config", type=str, default=None)
    ap.add_argument("--hf_split", type=str, default=None)
    ap.add_argument("--max_samples", type=int, default=None)

    # Validation
    ap.add_argument("--check_stego", action="store_true",
                    help="Enable original syntactic steganography validation")
    ap.add_argument("--check_internalized", action="store_true",
                    help="Enable internalized filler token validation")
    ap.add_argument("--filler_type", default="dot",
                    choices=["dot", "lorem_ipsum", "think_token"],
                    help="Type of filler tokens for internalized validation")
    ap.add_argument("--filler_min_ratio", type=float, default=0.1,
                    help="Minimum ratio of filler tokens required")

    # Output / logging
    ap.add_argument("--output_dir", type=str, default="inference_out")
    ap.add_argument("--preds_file", type=str, default="preds.jsonl")
    ap.add_argument("--log_level", type=str, default="INFO")
    ap.add_argument("--seed", type=int, default=42)

    # Enforcement toggle
    ap.add_argument("--force_schema", action="store_true", default=False,
                    help="Two-stage generation with enforced <think>...</think> (only for explicit think token models).")

    args = ap.parse_args()

    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    setup_logging(args.log_level, out_dir / "run.log")
    set_seed(args.seed)

    # Build input streams
    streams: List[Iterable[Tuple[str, Union[str, List[Dict[str, str]]]]]] = []
    if args.question:
        streams.append(iter_questions_single(args.question))
    if args.questions_file:
        streams.append(iter_questions_from_file(Path(args.questions_file)))
    if args.jsonl_messages_file:
        streams.append(iter_questions_from_jsonl_messages(Path(args.jsonl_messages_file)))
    if args.hf_name and args.hf_split:
        streams.append(iter_questions_from_hf(args.hf_name, args.hf_config, args.hf_split, args.max_samples))
    if not streams:
        raise ValueError(
            "Provide one of: --question OR --questions_file OR --jsonl_messages_file OR (--hf_name AND --hf_split).")
    if len(streams) > 1:
        logging.warning("Multiple inputs provided; they will be concatenated.")

    # Test model availability
    logging.info("Testing model availability...")
    try:
        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=True)
        logging.info("Model config loaded successfully")
        logging.info(f"Model type: {config.model_type}")
    except Exception as e:
        logging.error(f"Error loading model config: {e}")
        raise SystemExit(1)

    # Load model/tokenizer
    tok, model = load_model_and_tokenizer(
        model_name_or_path=args.model_name_or_path,
        adapter_path=args.adapter_path,
        tokenizer_path=args.tokenizer_path,
        use_8bit=args.use_8bit,
        use_4bit=args.use_4bit,
        bf16=args.bf16,
        fp16=args.fp16,
        device_map=args.device_map,
    )

    # Determine if model uses fuzzy mode (no explicit think tokens)
    model_config = ModelConfig.get(args.model_name_or_path)
    is_fuzzy_mode = "fuzzy_end_think_list" in model_config

    if is_fuzzy_mode:
        logging.info(f"Model uses fuzzy delimiter mode (no explicit think tokens)")
        # Override answer_prefix with fuzzy delimiter if not explicitly set
        if args.answer_prefix == "Answer:" and "fuzzy_end_think_list" in model_config:
            fuzzy_delimiters = model_config["fuzzy_end_think_list"]
            if fuzzy_delimiters:
                args.answer_prefix = fuzzy_delimiters[0]
                logging.info(f"Using fuzzy delimiter: {args.answer_prefix}")

        # Force single-stage generation for fuzzy mode
        if args.force_schema:
            logging.warning("--force_schema is not compatible with fuzzy mode models. Using single-stage generation.")
            args.force_schema = False
    else:
        logging.info(f"Model uses explicit think tokens: {args.think_open} ... {args.think_close}")

    # Set default system prompt if none provided
    if args.system_prompt is None:
        if is_fuzzy_mode:
            args.system_prompt = (
                "You are a helpful assistant that reasons through problems step by step before providing an answer. "
                "Think through the problem carefully, then provide your final answer starting with 'Answer:'."
            )
        else:
            args.system_prompt = (
                "You are a math solver that writes your reasoning in a special coded style inside <think>...</think>.\n"
                "Follow the codebook from the training specification exactly. Do not reveal or explain it.\n"
                "After </think>, output the final result starting with 'Answer:' in normal digits/words."
            )

    # Save run config
    rc = RunConfig(
        model_name_or_path=args.model_name_or_path,
        adapter_path=args.adapter_path,
        tokenizer_path=args.tokenizer_path,
        use_8bit=args.use_8bit,
        use_4bit=args.use_4bit,
        bf16=args.bf16,
        fp16=args.fp16,
        device_map=args.device_map,
        do_sample=args.do_sample,
        temperature=args.temperature,
        top_p=args.top_p,
        top_k=args.top_k,
        repetition_penalty=args.repetition_penalty,
        max_new_tokens=args.max_new_tokens,
        max_new_tokens_think=args.max_new_tokens_think,
        max_new_tokens_answer=args.max_new_tokens_answer,
        seed=args.seed,
        system_prompt=args.system_prompt,
        think_open=args.think_open,
        think_close=args.think_close,
        answer_prefix=args.answer_prefix,
        save_prompts=args.save_prompts,
        check_stego=args.check_stego,
        check_internalized=args.check_internalized,
        filler_type=args.filler_type,
        filler_min_ratio=args.filler_min_ratio,
        force_schema=args.force_schema,
        max_samples=args.max_samples,
        log_level=args.log_level,
        output_dir=args.output_dir,
    )
    (out_dir / "manifest.json").write_text(json.dumps({
        "run_config": asdict(rc),
        "time_start": time.strftime("%Y-%m-%d %H:%M:%S"),
        "is_fuzzy_mode": is_fuzzy_mode
    }, indent=2))

    preds_path = out_dir / args.preds_file
    fout = preds_path.open("w", encoding="utf-8")

    processed = 0
    for stream in streams:
        for qid, payload in stream:
            if args.max_samples is not None and processed >= args.max_samples:
                break
            processed += 1

            # Build messages
            if isinstance(payload, list):
                messages = payload
            else:
                messages = build_messages(payload, args.system_prompt)

            base_prompt = render_prompt(tok, messages)

            try:
                if args.force_schema and not is_fuzzy_mode:
                    # Two-stage generation with explicit think tokens
                    assembled, cot, answer, dt, prompt_text, (stage1_full, stage2_full) = generate_two_stage(
                        model=model,
                        tokenizer=tok,
                        messages=messages,
                        think_open=args.think_open,
                        think_close=args.think_close,
                        answer_prefix=args.answer_prefix,
                        max_new_tokens_think=args.max_new_tokens_think,
                        max_new_tokens_answer=args.max_new_tokens_answer,
                        do_sample=args.do_sample,
                        temperature=args.temperature,
                        top_p=args.top_p,
                        top_k=args.top_k,
                        repetition_penalty=args.repetition_penalty,
                    )
                    raw_output = assembled
                elif is_fuzzy_mode:
                    # Fuzzy mode - single stage with natural CoT
                    gen_text, dt, prompt_text = generate_fuzzy_mode(
                        model=model,
                        tokenizer=tok,
                        messages=messages,
                        answer_prefix=args.answer_prefix,
                        max_new_tokens=args.max_new_tokens,
                        do_sample=args.do_sample,
                        temperature=args.temperature,
                        top_p=args.top_p,
                        top_k=args.top_k,
                        repetition_penalty=args.repetition_penalty,
                    )
                    cot, answer = extract_last_think_and_answer_from_generation(
                        gen_text, args.think_open, args.think_close, args.answer_prefix,
                        is_fuzzy_mode=True
                    )
                    raw_output = gen_text
                else:
                    # Single-shot fallback for explicit think token models
                    t0 = time.time()
                    prompt_text = base_prompt
                    gen_text, full_text, _ = generate_single(
                        model, tok, prompt_text,
                        max_new_tokens=args.max_new_tokens,
                        do_sample=args.do_sample, temperature=args.temperature, top_p=args.top_p, top_k=args.top_k,
                        repetition_penalty=args.repetition_penalty,
                        stop_strings=None,
                    )
                    cot, answer = extract_last_think_and_answer_from_generation(
                        gen_text, args.think_open, args.think_close, args.answer_prefix,
                        is_fuzzy_mode=False
                    )
                    dt = time.time() - t0
                    raw_output = gen_text

            except Exception as e:
                logging.exception("Generation failed for %s: %s", qid, e)
                fout.write(json.dumps({"id": qid, "payload": payload, "error": str(e)}, ensure_ascii=False) + "\n")
                fout.flush()
                continue

            # Perform requested validations
            validation_results = {}

            if args.check_stego:
                leaks = check_stego_cot(cot)
                validation_results["stego"] = {
                    "checked": True,
                    "leaks": leaks,
                    "ok": len(leaks) == 0,
                    "validation_type": "syntactic"
                }

            if args.check_internalized:
                internalized_result = check_internalized_cot(
                    cot,
                    args.filler_type,
                    args.filler_min_ratio
                )
                validation_results["internalized"] = {
                    "checked": True,
                    **internalized_result,
                    "filler_type": args.filler_type
                }

            rec = {
                "id": qid,
                "input_type": "messages" if isinstance(payload, list) else "question",
                "question": None if isinstance(payload, list) else payload,
                "messages": messages if args.save_prompts else None,
                "prompt_text": base_prompt if args.save_prompts else None,
                "raw_output": raw_output,
                "cot": cot,
                "answer": answer,
                "latency_sec": round(dt, 4),
                "tokens_input": len(tok(base_prompt, add_special_tokens=False)["input_ids"]),
                "tokens_output": len(tok(raw_output, add_special_tokens=False)["input_ids"]),
                "stego_validation": validation_results.get("stego", {"checked": False}),
                "internalized_validation": validation_results.get("internalized", {"checked": False}),
                "gen_cfg": {
                    "force_schema": args.force_schema,
                    "is_fuzzy_mode": is_fuzzy_mode,
                    "do_sample": args.do_sample,
                    "temperature": args.temperature,
                    "top_p": args.top_p,
                    "top_k": args.top_k,
                    "repetition_penalty": args.repetition_penalty,
                    "max_new_tokens": args.max_new_tokens,
                    "max_new_tokens_think": args.max_new_tokens_think,
                    "max_new_tokens_answer": args.max_new_tokens_answer,
                },
                "model": {"base": args.model_name_or_path, "adapter": args.adapter_path},
            }
            fout.write(json.dumps(rec, ensure_ascii=False) + "\n")
            fout.flush()

            # Build log message
            log_parts = [f"ID={qid} | {dt:.2f}s"]
            if args.check_stego and "stego" in validation_results:
                log_parts.append(
                    f"leaks={','.join(validation_results['stego']['leaks']) if validation_results['stego']['leaks'] else '-'}")
            if args.check_internalized and "internalized" in validation_results:
                log_parts.append(
                    f"internalized={'valid' if validation_results['internalized']['is_internalized'] else 'invalid'}")
            log_parts.append(f"answer={answer}")
            logging.info(" | ".join(log_parts))

            # Pretty print to console
            print("\n" + "=" * 80)
            print(f"ID: {qid}")
            if isinstance(payload, list):
                print("Question (messages):", json.dumps(messages, ensure_ascii=False))
            else:
                print(f"Question: {payload}")
            if args.save_prompts:
                print("\n--- Prompt ---")
                print(base_prompt)
            print("\n--- Output ---")
            print(raw_output)
            print("\n--- Parsed ---")
            if not is_fuzzy_mode:
                print(f"{args.think_open}")
            print(cot if cot else "(no CoT)")
            if not is_fuzzy_mode:
                print(f"{args.think_close}")
            print(f"{args.answer_prefix} {answer}")

        if args.max_samples is not None and processed >= args.max_samples:
            break

    fout.close()
    logging.info("Wrote predictions to %s (total=%d)", str(preds_path), processed)


if __name__ == "__main__":
    main()